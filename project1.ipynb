{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ta mÃ¨re la pute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from proj1_helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x, mean_x=None , std_x=None):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    if mean_x==None:\n",
    "        mean_x = np.mean(x)\n",
    "        std_x = np.std(x)\n",
    "        \n",
    "    x = x - mean_x     \n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "\n",
    "def build_model_data(y, x):\n",
    "    \"\"\"Form (y,tX) to get regression data in matrix form.\"\"\"\n",
    "    num_samples = len(y)\n",
    "    tx = np.c_[np.ones(num_samples), x]\n",
    "    return y, tx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices of all data points that does'nt contain any values equal to -999 (unknown values)\n",
    "indices=np.all(tX>-999,axis=1)\n",
    "tX=tX[indices]\n",
    "y=y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX,tX_mean,tX_std=standardize(tX)\n",
    "y,tX =build_model_data(y,tX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the loss using mse\n",
    "def compute_loss(y, tx, w):\n",
    "    e=y-tx.dot(w)\n",
    "    n=len(y)\n",
    "    return 1/(2*n)*e.dot(e)\n",
    "\n",
    "#Compute the gradient of the loss function (mse)\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e=y-tx.dot(w)\n",
    "    grd=tx.T.dot(e)\n",
    "    return -(1/len(y))*grd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent algorithm\n",
    "def least_squares_GD(y, tx, initial_w,max_iters, gamma):\n",
    "\n",
    "    w=initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        grd=compute_gradient(y,tx,w)\n",
    "        w=w-gamma*grd\n",
    "        \n",
    "        loss=compute_loss(y,tx,w)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    \n",
    "    loss=compute_loss(y,tx,w)\n",
    "    return w,loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_least_squares_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"   \n",
    "    my=None\n",
    "    mx=None\n",
    "    for mmini_y,mini_x in batch_iter(y, tx, batch_size=1):\n",
    "            my=mmini_y\n",
    "            mx=mini_x\n",
    "    \n",
    "    return compute_gradient(my, mx, w)\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"Gradient descent algorithm\"\n",
    "    w=initial_w\n",
    "    for i in range(max_iters):\n",
    "        print(i)\n",
    "        grd=gamma*compute_stoch_least_squares_gradient(y,tx,w)\n",
    "        w=w-grd\n",
    "    loss=compute_loss(y,tx,w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=0.46432477976815745, w0=-0.005080893795695452, w1=-0.001627853453127974\n",
      "Gradient Descent(1/499): loss=0.4514018156604957, w0=-0.015607155269145077, w1=-0.005453818868881252\n",
      "Gradient Descent(2/499): loss=0.4461721267208156, w0=-0.015414911569037034, w1=-0.004269585821195729\n",
      "Gradient Descent(3/499): loss=0.4435862496672459, w0=-0.021322643132074312, w1=-0.005772148652084171\n",
      "Gradient Descent(4/499): loss=0.4419445141622309, w0=-0.019737586194176624, w1=-0.0038382449919124755\n",
      "Gradient Descent(5/499): loss=0.44066953863779956, w0=-0.02359233670605733, w1=-0.004355934608592177\n",
      "Gradient Descent(6/499): loss=0.4395609403066671, w0=-0.02183523467576719, w1=-0.00232718133489184\n",
      "Gradient Descent(7/499): loss=0.4385457740792175, w0=-0.024598833598555865, w1=-0.002354617005914637\n",
      "Gradient Descent(8/499): loss=0.4375948086023829, w0=-0.02301727954220666, w1=-0.0004223100043880862\n",
      "Gradient Descent(9/499): loss=0.43669431187204955, w0=-0.025094295988024574, w1=-0.0001622513854621221\n",
      "Gradient Descent(10/499): loss=0.4358364425606168, w0=-0.0237566613187068, w1=0.0016316528703820576\n",
      "Gradient Descent(11/499): loss=0.43501589784698336, w0=-0.025348568609941988, w1=0.0020817496481729996\n",
      "Gradient Descent(12/499): loss=0.4342286812305434, w0=-0.024239929907723567, w1=0.0037407610953050984\n",
      "Gradient Descent(13/499): loss=0.43347160150033787, w0=-0.02546691539786088, w1=0.004324800832994941\n",
      "Gradient Descent(14/499): loss=0.43274203397061284, w0=-0.02455244797693608, w1=0.005864099909645265\n",
      "Gradient Descent(15/499): loss=0.43203778361789774, w0=-0.02549684228942581, w1=0.006544721679166749\n",
      "Gradient Descent(16/499): loss=0.4313569936427486, w0=-0.024741341649583526, w1=0.00798059912609657\n",
      "Gradient Descent(17/499): loss=0.43069807829312506, w0=-0.025464293420711798, w1=0.008730616562081617\n",
      "Gradient Descent(18/499): loss=0.4300596710535234, w0=-0.024837378822251826, w1=0.010077526995652147\n",
      "Gradient Descent(19/499): loss=0.4294405837720146, w0=-0.025386148504151107, w1=0.01087627293262398\n",
      "Gradient Descent(20/499): loss=0.4288397740836044, w0=-0.024862957468487372, w1=0.012146311135564056\n",
      "Gradient Descent(21/499): loss=0.4282563193189519, w0=-0.02527472761565147, w1=0.012977782208716725\n",
      "Gradient Descent(22/499): loss=0.4276893955515522, w0=-0.024835322867910358, w1=0.014180872224258758\n",
      "Gradient Descent(23/499): loss=0.4271382607404731, w0=-0.025139567004869114, w1=0.015032603064489686\n",
      "Gradient Descent(24/499): loss=0.4266022411462418, w0=-0.024768086865504493, w1=0.016176848807631986\n",
      "Gradient Descent(25/499): loss=0.4260807203661711, w0=-0.024988235662176783, w1=0.017039132146269412\n",
      "Gradient Descent(26/499): loss=0.42557313046768946, w0=-0.024672096085267686, w1=0.018131161379705756\n",
      "Gradient Descent(27/499): loss=0.42507894480308633, w0=-0.0248267924257896, w1=0.018996465204935357\n",
      "Gradient Descent(28/499): loss=0.4245976721726038, w0=-0.024556014839565852, w1=0.020041723441784363\n",
      "Gradient Descent(29/499): loss=0.4241288520694498, w0=-0.02466009083755802, w1=0.02090423902408163\n",
      "Gradient Descent(30/499): loss=0.42367205079355424, w0=-0.024426757740363153, w1=0.02190722893337891\n",
      "Gradient Descent(31/499): loss=0.42322685826344714, w0=-0.024492006554690103, w1=0.022762514494700428\n",
      "Gradient Descent(32/499): loss=0.4227928853896592, w0=-0.024289827468362987, w1=0.023726987305638478\n",
      "Gradient Descent(33/499): loss=0.4223697619002508, w0=-0.024325616911831736, w1=0.024571685101029903\n",
      "Gradient Descent(34/499): loss=0.42195713453084055, w0=-0.024149584613578853, w1=0.02550079247361235\n",
      "Gradient Descent(35/499): loss=0.42155466550890547, w0=-0.024163346327638377, w1=0.02633240350701644\n",
      "Gradient Descent(36/499): loss=0.42116203127605795, w0=-0.02400946538154918, w1=0.027228817668929567\n",
      "Gradient Descent(37/499): loss=0.4207789214031458, w0=-0.024007085283059247, w1=0.028045522105078248\n",
      "Gradient Descent(38/499): loss=0.4204050376619476, w0=-0.02387215796347655, w1=0.028911530782945918\n",
      "Gradient Descent(39/499): loss=0.4200400932243712, w0=-0.023858288063055354, w1=0.029712044743865778\n",
      "Gradient Descent(40/499): loss=0.4196838119657885, w0=-0.023739745661178775, w1=0.030549626163968866\n",
      "Gradient Descent(41/499): loss=0.4193359278537159, w0=-0.0237180531569564, w1=0.03133308754852429\n",
      "Gradient Descent(42/499): loss=0.41899618440672115, w0=-0.023613823110984318, w1=0.03214396970941706\n",
      "Gradient Descent(43/499): loss=0.41866433421137866, w0=-0.023587189406733267, w1=0.03290984718171339\n",
      "Gradient Descent(44/499): loss=0.4183401384874511, w0=-0.023495590683626926, w1=0.0336955547265376\n",
      "Gradient Descent(45/499): loss=0.41802336669336915, w0=-0.02346627041409207, w1=0.03444357520593175\n",
      "Gradient Descent(46/499): loss=0.417713796165592, w0=-0.023385931159042264, w1=0.03520546652258774\n",
      "Gradient Descent(47/499): loss=0.4174112117866558, w0=-0.0233556792678489, w1=0.035935557450157855\n",
      "Gradient Descent(48/499): loss=0.4171154056776858, w0=-0.02328547199770409, w1=0.03667485407228602\n",
      "Gradient Descent(49/499): loss=0.41682617691194374, w0=-0.023255645290624724, w1=0.037387097478988975\n",
      "Gradient Descent(50/499): loss=0.4165433312466018, w0=-0.023194635904654905, w1=0.038104907421661\n",
      "Gradient Descent(51/499): loss=0.4162666808704501, w0=-0.023166274207058628, w1=0.03879950342198636\n",
      "Gradient Descent(52/499): loss=0.4159960441656516, w0=-0.023113681876321915, w1=0.03949683973936645\n",
      "Gradient Descent(53/499): loss=0.41573124548198537, w0=-0.023087572891104736, w1=0.04017407755212153\n",
      "Gradient Descent(54/499): loss=0.41547211492228786, w0=-0.0230427385098103, w1=0.04085187313091081\n",
      "Gradient Descent(55/499): loss=0.415218488138014, w0=-0.023019469647921106, w1=0.041512108110341\n",
      "Gradient Descent(56/499): loss=0.41497020613401575, w0=-0.022981831021199467, w1=0.0421712274972217\n",
      "Gradient Descent(57/499): loss=0.41472711508177557, w0=-0.022961830818862806, w1=0.04281486296243868\n",
      "Gradient Descent(58/499): loss=0.4144890661404441, w0=-0.022930903148762224, w1=0.04345611185381481\n",
      "Gradient Descent(59/499): loss=0.4142559152851305, w0=-0.02291447436007274, w1=0.04408358474796882\n",
      "Gradient Descent(60/499): loss=0.41402752314196095, w0=-0.02288983489716958, w1=0.04470771763642511\n",
      "Gradient Descent(61/499): loss=0.4138037548294913, w0=-0.022877180931123665, w1=0.04531948724157237\n",
      "Gradient Descent(62/499): loss=0.4135844798061056, w0=-0.022858456900083368, w1=0.045927213608030856\n",
      "Gradient Descent(63/499): loss=0.4133695717230754, w0=-0.02284970293596846, w1=0.046523752697073005\n",
      "Gradient Descent(64/499): loss=0.41315890828299134, w0=-0.022836562033332113, w1=0.047115742054604844\n",
      "Gradient Descent(65/499): loss=0.41295237110330313, w0=-0.022831771880678822, w1=0.047697529985882885\n",
      "Gradient Descent(66/499): loss=0.4127498455847347, w0=-0.02282391479283235, w1=0.0482744160157853\n",
      "Gradient Descent(67/499): loss=0.41255122078435535, w0=-0.02282310434825639, w1=0.048841933375177955\n",
      "Gradient Descent(68/499): loss=0.41235638929310864, w0=-0.02282025885545985, w1=0.049404317344507315\n",
      "Gradient Descent(69/499): loss=0.4121652471176144, w0=-0.0228234068378316, w1=0.04995804181922788\n",
      "Gradient Descent(70/499): loss=0.4119776935660731, w0=-0.022825323163054547, w1=0.050506495428534394\n",
      "Gradient Descent(71/499): loss=0.4117936311381105, w0=-0.02283237967187669, w1=0.051046898660246684\n",
      "Gradient Descent(72/499): loss=0.4116129654184118, w0=-0.022838826806298684, w1=0.05158196643845275\n",
      "Gradient Descent(73/499): loss=0.4114356049740042, w0=-0.022849720139035006, w1=0.052109511654033186\n",
      "Gradient Descent(74/499): loss=0.41126146125505025, w0=-0.02286048293361048, w1=0.05263171299239849\n",
      "Gradient Descent(75/499): loss=0.4110904484990263, w0=-0.022875125010475124, w1=0.05314685325121263\n",
      "Gradient Descent(76/499): loss=0.4109224836381625, w0=-0.022890001868228475, w1=0.053656684148685255\n",
      "Gradient Descent(77/499): loss=0.4107574862100277, w0=-0.022908292543208943, w1=0.05415986107766355\n",
      "Gradient Descent(78/499): loss=0.4105953782711488, w0=-0.022927093582523338, w1=0.054657795654485905\n",
      "Gradient Descent(79/499): loss=0.4104360843135565, w0=-0.022948924063655795, w1=0.05514943856820501\n",
      "Gradient Descent(80/499): loss=0.41027953118415617, w0=-0.02297146965080104, w1=0.05563593039252652\n",
      "Gradient Descent(81/499): loss=0.41012564800682516, w0=-0.022996725208133464, w1=0.056116455716232146\n",
      "Gradient Descent(82/499): loss=0.4099743661071422, w0=-0.023022844779264716, w1=0.056591938978968866\n",
      "Gradient Descent(83/499): loss=0.409825618939657, w0=-0.023051406883289088, w1=0.05706174990905398\n",
      "Gradient Descent(84/499): loss=0.40967934201761436, w0=-0.0230809379934155, w1=0.05752664047477102\n",
      "Gradient Descent(85/499): loss=0.4095354728450472, w0=-0.023112685998232282, w1=0.057986126824481406\n",
      "Gradient Descent(86/499): loss=0.40939395085115826, w0=-0.023145473548209557, w1=0.058440823180217503\n",
      "Gradient Descent(87/499): loss=0.4092547173269113, w0=-0.023180286010874906, w1=0.05889036136896011\n",
      "Gradient Descent(88/499): loss=0.40911771536375685, w0=-0.023216181614111098, w1=0.059335245488320414\n",
      "Gradient Descent(89/499): loss=0.4089828897944203, w0=-0.023253937323365326, w1=0.059775198641429425\n",
      "Gradient Descent(90/499): loss=0.4088501871356803, w0=-0.023292798782269242, w1=0.06021063677767001\n",
      "Gradient Descent(91/499): loss=0.40871955553307293, w0=-0.023333377555240595, w1=0.06064135491026708\n",
      "Gradient Descent(92/499): loss=0.40859094470745216, w0=-0.023375068423978927, w1=0.06106769832926979\n",
      "Gradient Descent(93/499): loss=0.4084643059033475, w0=-0.02341835171776878, w1=0.06148951859327683\n",
      "Gradient Descent(94/499): loss=0.40833959183905405, w0=-0.023462740933018363, w1=0.06190710425509808\n",
      "Gradient Descent(95/499): loss=0.4082167566584005, w0=-0.02350861230871908, w1=0.06232035123279625\n",
      "Gradient Descent(96/499): loss=0.4080957558841333, w0=-0.02355557387410902, w1=0.06272950242873587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(97/499): loss=0.40797654637286646, w0=-0.02360391934331698, w1=0.06313448845972944\n",
      "Gradient Descent(98/499): loss=0.4078590862715419, w0=-0.023653332056391384, w1=0.06353551541050308\n",
      "Gradient Descent(99/499): loss=0.40774333497534854, w0=-0.02370404033428211, w1=0.06393254094171591\n",
      "Gradient Descent(100/499): loss=0.4076292530870533, w0=-0.023755787547265764, w1=0.06432574136124455\n",
      "Gradient Descent(101/499): loss=0.4075168023776932, w0=-0.02380875023149733, w1=0.06471509531178891\n",
      "Gradient Descent(102/499): loss=0.40740594574858446, w0=-0.023862719639062297, w1=0.06510075494027916\n",
      "Gradient Descent(103/499): loss=0.407296647194604, w0=-0.023917831329928446, w1=0.06548271507480244\n",
      "Gradient Descent(104/499): loss=0.40718887176869867, w0=-0.023973914778654997, w1=0.06586110818413045\n",
      "Gradient Descent(105/499): loss=0.40708258554758364, w0=-0.024031073152830262, w1=0.06623594148965517\n",
      "Gradient Descent(106/499): loss=0.4069777555985855, w0=-0.02408916646822208, w1=0.06660733136354699\n",
      "Gradient Descent(107/499): loss=0.4068743499475965, w0=-0.024148272315974478, w1=0.06697529442594272\n",
      "Gradient Descent(108/499): loss=0.406772337548098, w0=-0.024208275143797725, w1=0.06733993381703346\n",
      "Gradient Descent(109/499): loss=0.40667168825121897, w0=-0.024269232377568634, w1=0.06770127319415489\n",
      "Gradient Descent(110/499): loss=0.4065723727767947, w0=-0.02433104803699264, w1=0.06805940475968444\n",
      "Gradient Descent(111/499): loss=0.40647436268539194, w0=-0.024393763677660253, w1=0.06841435734892214\n",
      "Gradient Descent(112/499): loss=0.40637763035126645, w0=-0.02445729902422888, w1=0.06876621406656798\n",
      "Gradient Descent(113/499): loss=0.4062821489362232, w0=-0.024521683170103063, w1=0.06911500746512418\n",
      "Gradient Descent(114/499): loss=0.40618789236434855, w0=-0.024586848466994594, w1=0.06946081303026629\n",
      "Gradient Descent(115/499): loss=0.4060948352975829, w0=-0.02465281424957436, w1=0.06980366588691755\n",
      "Gradient Descent(116/499): loss=0.40600295311210916, w0=-0.024719523045940595, w1=0.07014363509246672\n",
      "Gradient Descent(117/499): loss=0.40591222187552545, w0=-0.024786986575650923, w1=0.07048075744993006\n",
      "Gradient Descent(118/499): loss=0.405822618324779, w0=-0.02485515559108487, w1=0.07081509654971853\n",
      "Gradient Descent(119/499): loss=0.40573411984483326, w0=-0.024924035895556543, w1=0.07114669017701836\n",
      "Gradient Descent(120/499): loss=0.405646704448045, w0=-0.024993584909938464, w1=0.07147559723364458\n",
      "Gradient Descent(121/499): loss=0.40556035075422614, w0=-0.025063803866871553, w1=0.07180185594809858\n",
      "Gradient Descent(122/499): loss=0.40547503797136847, w0=-0.025134655614998714, w1=0.07212552116603062\n",
      "Gradient Descent(123/499): loss=0.4053907458770084, w0=-0.02520613788123099, w1=0.07244663114464514\n",
      "Gradient Descent(124/499): loss=0.4053074548002096, w0=-0.025278217951756676, w1=0.07276523718931484\n",
      "Gradient Descent(125/499): loss=0.4052251456041435, w0=-0.025350890889822272, w1=0.07308137726951514\n",
      "Gradient Descent(126/499): loss=0.4051437996692485, w0=-0.025424127628122857, w1=0.07339509957307522\n",
      "Gradient Descent(127/499): loss=0.40506339887694465, w0=-0.025497921231317267, w1=0.07370644154280015\n",
      "Gradient Descent(128/499): loss=0.4049839255938902, w0=-0.025572245645978348, w1=0.0740154485971657\n",
      "Gradient Descent(129/499): loss=0.40490536265675775, w0=-0.02564709246273035, w1=0.07432215747443627\n",
      "Gradient Descent(130/499): loss=0.4048276933575144, w0=-0.025722438135398474, w1=0.07462661111218909\n",
      "Gradient Descent(131/499): loss=0.4047509014291887, w0=-0.02579827319357672, w1=0.07492884541432061\n",
      "Gradient Descent(132/499): loss=0.4046749710321078, w0=-0.0258745761919667, w1=0.07522890107801743\n",
      "Gradient Descent(133/499): loss=0.4045998867405886, w0=-0.025951336923610593, w1=0.07552681308069081\n",
      "Gradient Descent(134/499): loss=0.40452563353006876, w0=-0.026028535717491582, w1=0.07582262008108415\n",
      "Gradient Descent(135/499): loss=0.40445219676466204, w0=-0.02610616188434544, w1=0.07611635606752405\n",
      "Gradient Descent(136/499): loss=0.40437956218512294, w0=-0.02618419726435505, w1=0.07640805783117581\n",
      "Gradient Descent(137/499): loss=0.4043077158972093, w0=-0.02626263088449611, w1=0.07669775833170753\n",
      "Gradient Descent(138/499): loss=0.4042366443604262, w0=-0.02634144588365187, w1=0.07698549263844943\n",
      "Gradient Descent(139/499): loss=0.40416633437714183, w0=-0.02642063115943201, w1=0.07727129266072157\n",
      "Gradient Descent(140/499): loss=0.40409677308205894, w0=-0.026500170977225224, w1=0.07755519187139379\n",
      "Gradient Descent(141/499): loss=0.4040279479320345, w0=-0.026580054224689988, w1=0.07783722112156397\n",
      "Gradient Descent(142/499): loss=0.40395984669623225, w0=-0.026660266153659396, w1=0.07811741239644182\n",
      "Gradient Descent(143/499): loss=0.40389245744659746, w0=-0.02674079573356284, w1=0.07839579549162734\n",
      "Gradient Descent(144/499): loss=0.40382576854864555, w0=-0.02682162908825505, w1=0.07867240099992687\n",
      "Gradient Descent(145/499): loss=0.4037597686525512, w0=-0.02690275533875332, w1=0.07894725767222405\n",
      "Gradient Descent(146/499): loss=0.4036944466845293, w0=-0.026984161386984805, w1=0.07922039479305881\n",
      "Gradient Descent(147/499): loss=0.40362979183849773, w0=-0.02706583655806287, w1=0.07949184008543307\n",
      "Gradient Descent(148/499): loss=0.4035657935680124, w0=-0.027147768454404454, w1=0.07976162160057819\n",
      "Gradient Descent(149/499): loss=0.4035024415784655, w0=-0.027229946644068016, w1=0.08002976605492348\n",
      "Gradient Descent(150/499): loss=0.40343972581953824, w0=-0.027312359365477712, w1=0.0802963003337268\n",
      "Gradient Descent(151/499): loss=0.4033776364778999, w0=-0.02739499645772455, w1=0.080561250171388\n",
      "Gradient Descent(152/499): loss=0.4033161639701431, w0=-0.0274778467412589, w1=0.08082464134815368\n",
      "Gradient Descent(153/499): loss=0.4032552989359505, w0=-0.027560900345830003, w1=0.08108649864319871\n",
      "Gradient Descent(154/499): loss=0.4031950322314832, w0=-0.02764414662836737, w1=0.08134684678735436\n",
      "Gradient Descent(155/499): loss=0.4031353549229823, w0=-0.0277275760222673, w1=0.08160570963287607\n",
      "Gradient Descent(156/499): loss=0.40307625828057947, w0=-0.027811178382179613, w1=0.08186311091222138\n",
      "Gradient Descent(157/499): loss=0.403017733772308, w0=-0.027894944452947188, w1=0.08211907357994087\n",
      "Gradient Descent(158/499): loss=0.4029597730583066, w0=-0.027978864553659156, w1=0.08237362041726297\n",
      "Gradient Descent(159/499): loss=0.4029023679852111, w0=-0.028062929744362957, w1=0.08262677351069808\n",
      "Gradient Descent(160/499): loss=0.4028455105807278, w0=-0.028147130779740015, w1=0.08287855473402671\n",
      "Gradient Descent(161/499): loss=0.40278919304838084, w0=-0.028231459035668382, w1=0.08312898533548059\n",
      "Gradient Descent(162/499): loss=0.40273340776242966, w0=-0.028315905677176754, w1=0.08337808632224508\n",
      "Gradient Descent(163/499): loss=0.4026781472629495, w0=-0.028400462394188246, w1=0.08362587813386123\n",
      "Gradient Descent(164/499): loss=0.40262340425106996, w0=-0.028485120739772225, w1=0.0838723809491998\n",
      "Gradient Descent(165/499): loss=0.4025691715843673, w0=-0.02856987271426983, w1=0.08411761442832119\n",
      "Gradient Descent(166/499): loss=0.40251544227240327, w0=-0.028654710238893287, w1=0.084361597957783\n",
      "Gradient Descent(167/499): loss=0.40246220947240774, w0=-0.028739625619383858, w1=0.0846043504468444\n",
      "Gradient Descent(168/499): loss=0.40240946648509895, w0=-0.028824611127184405, w1=0.08484589052371455\n",
      "Gradient Descent(169/499): loss=0.4023572067506372, w0=-0.0289096593673835, w1=0.08508623637488844\n",
      "Gradient Descent(170/499): loss=0.40230542384470824, w0=-0.028994762945389465, w1=0.08532540590235675\n",
      "Gradient Descent(171/499): loss=0.4022541114747304, w0=-0.029079914758831038, w1=0.08556341659716493\n",
      "Gradient Descent(172/499): loss=0.4022032634761838, w0=-0.029165107732192767, w1=0.08580028566554981\n",
      "Gradient Descent(173/499): loss=0.4021528738090545, w0=-0.029250335048302916, w1=0.0860360299296448\n",
      "Gradient Descent(174/499): loss=0.40210293655439283, w0=-0.02933558993699129, w1=0.08627066592887507\n",
      "Gradient Descent(175/499): loss=0.40205344591097997, w0=-0.02942086585858505, w1=0.08650420984218711\n",
      "Gradient Descent(176/499): loss=0.4020043961921001, w0=-0.029506156335511617, w1=0.08673667756973606\n",
      "Gradient Descent(177/499): loss=0.4019557818224148, w0=-0.029591455097672217, w1=0.08696808467217411\n",
      "Gradient Descent(178/499): loss=0.4019075973349354, w0=-0.02967675594818656, w1=0.08719844643663195\n",
      "Gradient Descent(179/499): loss=0.4018598373680912, w0=-0.02976205287848681, w1=0.08742777782951935\n",
      "Gradient Descent(180/499): loss=0.40181249666288926, w0=-0.02984733996120801, w1=0.08765609354998272\n",
      "Gradient Descent(181/499): loss=0.401765570060164, w0=-0.029932611441234193, w1=0.08788340799340116\n",
      "Gradient Descent(182/499): loss=0.4017190524979122, w0=-0.03001786165017458, w1=0.08810973529485061\n",
      "Gradient Descent(183/499): loss=0.4016729390087112, w0=-0.030103085078313673, w1=0.08833508930105881\n",
      "Gradient Descent(184/499): loss=0.4016272247172194, w0=-0.030188276306254345, w1=0.08855948360588846\n",
      "Gradient Descent(185/499): loss=0.40158190483775213, w0=-0.030273430061706436, w1=0.08878293152897547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(186/499): loss=0.4015369746719343, w0=-0.03035854116478495, w1=0.08900544614483195\n",
      "Gradient Descent(187/499): loss=0.40149242960642556, w0=-0.030443604572763263, w1=0.08922704026675833\n",
      "Gradient Descent(188/499): loss=0.40144826511071685, w0=-0.030528615336235333, w1=0.0894477264708398\n",
      "Gradient Descent(189/499): loss=0.4014044767349929, w0=-0.030613568634317318, w1=0.08966751708401373\n",
      "Gradient Descent(190/499): loss=0.4013610601080628, w0=-0.030698459739455373, w1=0.08988642420397332\n",
      "Gradient Descent(191/499): loss=0.40131801093535296, w0=-0.030783284045049108, w1=0.09010445969050276\n",
      "Gradient Descent(192/499): loss=0.40127532499696306, w0=-0.03086803703714163, w1=0.09032163518209506\n",
      "Gradient Descent(193/499): loss=0.40123299814577934, w0=-0.03095271431603271, w1=0.09053796208985118\n",
      "Gradient Descent(194/499): loss=0.4011910263056482, w0=-0.031037311573449404, w1=0.09075345161145451\n",
      "Gradient Descent(195/499): loss=0.4011494054696033, w0=-0.03112182460939445, w1=0.0909681147270761\n",
      "Gradient Descent(196/499): loss=0.4011081316981487, w0=-0.031206249313683293, w1=0.09118196221121774\n",
      "Gradient Descent(197/499): loss=0.40106720111759175, w0=-0.03129058167901708, w1=0.09139500463018115\n",
      "Gradient Descent(198/499): loss=0.4010266099184292, w0=-0.03137481778600035, w1=0.0916072523521877\n",
      "Gradient Descent(199/499): loss=0.4009863543537796, w0=-0.031458953813224486, w1=0.09181871554606147\n",
      "Gradient Descent(200/499): loss=0.4009464307378647, w0=-0.031542986025061864, w1=0.09202940418995115\n",
      "Gradient Descent(201/499): loss=0.4009068354445357, w0=-0.03162691077938389, w1=0.09223932807094984\n",
      "Gradient Descent(202/499): loss=0.4008675649058435, w0=-0.03171072451757166, w1=0.09244849679267896\n",
      "Gradient Descent(203/499): loss=0.4008286156106532, w0=-0.03179442377036427, w1=0.0926569197756258\n",
      "Gradient Descent(204/499): loss=0.4007899841032975, w0=-0.03187800514964062, w1=0.09286460626379696\n",
      "Gradient Descent(205/499): loss=0.400751666982273, w0=-0.03196146535279162, w1=0.09307156532560062\n",
      "Gradient Descent(206/499): loss=0.4007136608989721, w0=-0.03204480115591892, w1=0.09327780585973558\n",
      "Gradient Descent(207/499): loss=0.4006759625564559, w0=-0.032128009417043454, w1=0.09348333659648184\n",
      "Gradient Descent(208/499): loss=0.40063856870825965, w0=-0.03221108707043934, w1=0.09368816610295803\n",
      "Gradient Descent(209/499): loss=0.4006014761572365, w0=-0.03229403112892672, w1=0.09389230278471306\n",
      "Gradient Descent(210/499): loss=0.4005646817544332, w0=-0.03237683867911659, w1=0.09409575489045843\n",
      "Gradient Descent(211/499): loss=0.4005281823979994, w0=-0.03245950688298476, w1=0.0942985305138765\n",
      "Gradient Descent(212/499): loss=0.4004919750321288, w0=-0.032542032973849194, w1=0.09450063759791351\n",
      "Gradient Descent(213/499): loss=0.4004560566460305, w0=-0.03262441425738107, w1=0.0947020839367381\n",
      "Gradient Descent(214/499): loss=0.40042042427293134, w0=-0.03270664810817253, w1=0.09490287717966406\n",
      "Gradient Descent(215/499): loss=0.40038507498910536, w0=-0.03278873197030866, w1=0.0951030248332075\n",
      "Gradient Descent(216/499): loss=0.40035000591293246, w0=-0.03287066335441248, w1=0.0953025342646948\n",
      "Gradient Descent(217/499): loss=0.400315214203984, w0=-0.032952439837875845, w1=0.0955014127043783\n",
      "Gradient Descent(218/499): loss=0.40028069706213376, w0=-0.03303405906229147, w1=0.09569966724877452\n",
      "Gradient Descent(219/499): loss=0.4002464517266946, w0=-0.03311551873342051, w1=0.0958973048628071\n",
      "Gradient Descent(220/499): loss=0.40021247547558036, w0=-0.03319681661893961, w1=0.09609433238291196\n",
      "Gradient Descent(221/499): loss=0.40017876562449, w0=-0.03327795054820664, w1=0.09629075651918338\n",
      "Gradient Descent(222/499): loss=0.40014531952611576, w0=-0.033358918410265385, w1=0.09648658385827602\n",
      "Gradient Descent(223/499): loss=0.4001121345693735, w0=-0.033439718153458024, w1=0.09668182086553584\n",
      "Gradient Descent(224/499): loss=0.40007920817865406, w0=-0.03352034778364164, w1=0.09687647388772327\n",
      "Gradient Descent(225/499): loss=0.40004653781309557, w0=-0.033600805363685766, w1=0.09707054915511526\n",
      "Gradient Descent(226/499): loss=0.40001412096587685, w0=-0.03368108901186407, w1=0.0972640527840696\n",
      "Gradient Descent(227/499): loss=0.3999819551635289, w0=-0.03376119690126731, w1=0.09745699077908777\n",
      "Gradient Descent(228/499): loss=0.3999500379652655, w0=-0.03384112725834073, w1=0.0976493690352372\n",
      "Gradient Descent(229/499): loss=0.39991836696233435, w0=-0.03392087836223631, w1=0.0978411933401672\n",
      "Gradient Descent(230/499): loss=0.3998869397773825, w0=-0.03400044854347233, w1=0.09803246937640307\n",
      "Gradient Descent(231/499): loss=0.39985575406384166, w0=-0.03407983618324362, w1=0.0982232027233098\n",
      "Gradient Descent(232/499): loss=0.3998248075053286, w0=-0.03415903971218437, w1=0.09841339885926957\n",
      "Gradient Descent(233/499): loss=0.39979409781506325, w0=-0.03423805760965111, w1=0.09860306316358962\n",
      "Gradient Descent(234/499): loss=0.39976362273530036, w0=-0.034316888402573485, w1=0.09879220091857299\n",
      "Gradient Descent(235/499): loss=0.399733380036778, w0=-0.03439553066472137, w1=0.09898081731136812\n",
      "Gradient Descent(236/499): loss=0.3997033675181798, w0=-0.03447398301563139, w1=0.09916891743594128\n",
      "Gradient Descent(237/499): loss=0.39967358300561207, w0=-0.034552244119867084, w1=0.09935650629486661\n",
      "Gradient Descent(238/499): loss=0.39964402435209406, w0=-0.03463031268601113, w1=0.0995435888012074\n",
      "Gradient Descent(239/499): loss=0.39961468943706185, w0=-0.0347081874659255, w1=0.09973016978024608\n",
      "Gradient Descent(240/499): loss=0.39958557616588514, w0=-0.03478586725380129, w1=0.09991625397128061\n",
      "Gradient Descent(241/499): loss=0.3995566824693965, w0=-0.03486335088542416, w1=0.1001018460292945\n",
      "Gradient Descent(242/499): loss=0.39952800630343305, w0=-0.03494063723727513, w1=0.10028695052667376\n",
      "Gradient Descent(243/499): loss=0.3994995456483893, w0=-0.03501772522580531, w1=0.10047157195481758\n",
      "Gradient Descent(244/499): loss=0.3994712985087817, w0=-0.03509461380658234, w1=0.10065571472578061\n",
      "Gradient Descent(245/499): loss=0.39944326291282517, w0=-0.035171301973577374, w1=0.10083938317382522\n",
      "Gradient Descent(246/499): loss=0.3994154369120184, w0=-0.0352477887583526, w1=0.10102258155699342\n",
      "Gradient Descent(247/499): loss=0.39938781858074107, w0=-0.03532407322936285, w1=0.10120531405860192\n",
      "Gradient Descent(248/499): loss=0.39936040601586076, w0=-0.035400154491180474, w1=0.10138758478874746\n",
      "Gradient Descent(249/499): loss=0.3993331973363496, w0=-0.035476031683813115, w1=0.10156939778574604\n",
      "Gradient Descent(250/499): loss=0.3993061906829099, w0=-0.03555170398196285, w1=0.10175075701757529\n",
      "Gradient Descent(251/499): loss=0.3992793842176103, w0=-0.03562717059436132, w1=0.10193166638325923\n",
      "Gradient Descent(252/499): loss=0.3992527761235286, w0=-0.03570243076306045, w1=0.10211212971425085\n",
      "Gradient Descent(253/499): loss=0.3992263646044061, w0=-0.035777483762785736, w1=0.10229215077576406\n",
      "Gradient Descent(254/499): loss=0.39920014788430763, w0=-0.035852328900256254, w1=0.10247173326809937\n",
      "Gradient Descent(255/499): loss=0.39917412420729154, w0=-0.0359269655135566, w1=0.1026508808279247\n",
      "Gradient Descent(256/499): loss=0.3991482918370866, w0=-0.03600139297148427, w1=0.10282959702954676\n",
      "Gradient Descent(257/499): loss=0.39912264905677636, w0=-0.03607561067294044, w1=0.10300788538614251\n",
      "Gradient Descent(258/499): loss=0.399097194168493, w0=-0.03614961804630314, w1=0.10318574935097873\n",
      "Gradient Descent(259/499): loss=0.3990719254931151, w0=-0.03622341454883681, w1=0.10336319231859563\n",
      "Gradient Descent(260/499): loss=0.3990468413699751, w0=-0.03629699966608983, w1=0.10354021762597714\n",
      "Gradient Descent(261/499): loss=0.3990219401565722, w0=-0.03637037291132285, w1=0.10371682855368836\n",
      "Gradient Descent(262/499): loss=0.3989972202282925, w0=-0.0364435338249293, w1=0.10389302832699851\n",
      "Gradient Descent(263/499): loss=0.398972679978135, w0=-0.036516481973882305, w1=0.10406882011697412\n",
      "Gradient Descent(264/499): loss=0.39894831781644435, w0=-0.036589216951177075, w1=0.10424420704155664\n",
      "Gradient Descent(265/499): loss=0.39892413217064876, w0=-0.03666173837529606, w1=0.10441919216661283\n",
      "Gradient Descent(266/499): loss=0.39890012148500503, w0=-0.036734045889672204, w1=0.10459377850696916\n",
      "Gradient Descent(267/499): loss=0.3988762842203483, w0=-0.03680613916217205, w1=0.10476796902742092\n",
      "Gradient Descent(268/499): loss=0.39885261885384754, w0=-0.036878017884578854, w1=0.1049417666437252\n",
      "Gradient Descent(269/499): loss=0.3988291238787664, w0=-0.03694968177209327, w1=0.10511517422357035\n",
      "Gradient Descent(270/499): loss=0.39880579780422987, w0=-0.03702113056283553, w1=0.1052881945875292\n",
      "Gradient Descent(271/499): loss=0.3987826391549942, w0=-0.037092364017363155, w1=0.10546083050999033\n",
      "Gradient Descent(272/499): loss=0.3987596464712248, w0=-0.037163381918191546, w1=0.10563308472007334\n",
      "Gradient Descent(273/499): loss=0.39873681830827556, w0=-0.03723418406932824, w1=0.10580495990252331\n",
      "Gradient Descent(274/499): loss=0.398714153236475, w0=-0.03730477029581111, w1=0.10597645869858956\n",
      "Gradient Descent(275/499): loss=0.39869164984091665, w0=-0.03737514044325883, w1=0.10614758370688492\n",
      "Gradient Descent(276/499): loss=0.3986693067212525, w0=-0.03744529437742588, w1=0.10631833748422936\n",
      "Gradient Descent(277/499): loss=0.3986471224914928, w0=-0.03751523198376883, w1=0.10648872254647543\n",
      "Gradient Descent(278/499): loss=0.3986250957798079, w0=-0.03758495316701755, w1=0.10665874136931826\n",
      "Gradient Descent(279/499): loss=0.3986032252283358, w0=-0.03765445785075672, w1=0.10682839638908835\n",
      "Gradient Descent(280/499): loss=0.39858150949299254, w0=-0.03772374597701272, w1=0.10699769000352946\n",
      "Gradient Descent(281/499): loss=0.39855994724328714, w0=-0.03779281750584995, w1=0.10716662457255996\n",
      "Gradient Descent(282/499): loss=0.39853853716213905, w0=-0.03786167241497272, w1=0.10733520241901981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(283/499): loss=0.39851727794570163, w0=-0.03793031069933598, w1=0.10750342582940184\n",
      "Gradient Descent(284/499): loss=0.3984961683031857, w0=-0.037998732370761665, w1=0.10767129705456906\n",
      "Gradient Descent(285/499): loss=0.3984752069566897, w0=-0.03806693745756336, w1=0.10783881831045694\n",
      "Gradient Descent(286/499): loss=0.3984543926410312, w0=-0.03813492600417661, w1=0.10800599177876229\n",
      "Gradient Descent(287/499): loss=0.39843372410358274, w0=-0.038202698070797056, w1=0.10817281960761771\n",
      "Gradient Descent(288/499): loss=0.3984132001041097, w0=-0.038270253733024215, w1=0.10833930391225312\n",
      "Gradient Descent(289/499): loss=0.3983928194146125, w0=-0.038337593081512654, w1=0.10850544677564355\n",
      "Gradient Descent(290/499): loss=0.39837258081917093, w0=-0.03840471622162876, w1=0.10867125024914438\n",
      "Gradient Descent(291/499): loss=0.3983524831137915, w0=-0.038471623273114514, w1=0.10883671635311348\n",
      "Gradient Descent(292/499): loss=0.39833252510625816, w0=-0.03853831436975684, w1=0.10900184707752122\n",
      "Gradient Descent(293/499): loss=0.3983127056159848, w0=-0.03860478965906351, w1=0.10916664438254803\n",
      "Gradient Descent(294/499): loss=0.3982930234738714, w0=-0.03867104930194468, w1=0.10933111019917012\n",
      "Gradient Descent(295/499): loss=0.39827347752216213, w0=-0.03873709347240049, w1=0.10949524642973343\n",
      "Gradient Descent(296/499): loss=0.39825406661430673, w0=-0.03880292235721433, w1=0.10965905494851613\n",
      "Gradient Descent(297/499): loss=0.3982347896148232, w0=-0.03886853615565188, w1=0.10982253760227972\n",
      "Gradient Descent(298/499): loss=0.39821564539916443, w0=-0.03893393507916556, w1=0.10998569621080932\n",
      "Gradient Descent(299/499): loss=0.39819663285358553, w0=-0.03899911935110462, w1=0.11014853256744292\n",
      "Gradient Descent(300/499): loss=0.3981777508750147, w0=-0.03906408920643052, w1=0.11031104843959015\n",
      "Gradient Descent(301/499): loss=0.39815899837092633, w0=-0.03912884489143761, w1=0.11047324556924064\n",
      "Gradient Descent(302/499): loss=0.3981403742592149, w0=-0.039193386663479005, w1=0.11063512567346216\n",
      "Gradient Descent(303/499): loss=0.3981218774680735, w0=-0.03925771479069762, w1=0.11079669044488885\n",
      "Gradient Descent(304/499): loss=0.3981035069358711, w0=-0.039321829551762104, w1=0.11095794155219961\n",
      "Gradient Descent(305/499): loss=0.39808526161103547, w0=-0.03938573123560788, w1=0.11111888064058688\n",
      "Gradient Descent(306/499): loss=0.39806714045193503, w0=-0.039449420141182856, w1=0.11127950933221616\n",
      "Gradient Descent(307/499): loss=0.3980491424267645, w0=-0.03951289657719801, w1=0.11143982922667614\n",
      "Gradient Descent(308/499): loss=0.39803126651343185, w0=-0.03957616086188262, w1=0.11159984190141999\n",
      "Gradient Descent(309/499): loss=0.3980135116994467, w0=-0.03963921332274409, w1=0.11175954891219762\n",
      "Gradient Descent(310/499): loss=0.3979958769818115, w0=-0.0397020542963323, w1=0.1119189517934795\n",
      "Gradient Descent(311/499): loss=0.39797836136691334, w0=-0.039764684128008425, w1=0.1120780520588717\n",
      "Gradient Descent(312/499): loss=0.39796096387041824, w0=-0.039827103171718045, w1=0.11223685120152294\n",
      "Gradient Descent(313/499): loss=0.3979436835171668, w0=-0.039889311789768664, w1=0.11239535069452317\n",
      "Gradient Descent(314/499): loss=0.3979265193410717, w0=-0.03995131035261127, w1=0.11255355199129438\n",
      "Gradient Descent(315/499): loss=0.39790947038501645, w0=-0.04001309923862623, w1=0.11271145652597343\n",
      "Gradient Descent(316/499): loss=0.39789253570075617, w0=-0.0400746788339131, w1=0.11286906571378731\n",
      "Gradient Descent(317/499): loss=0.3978757143488196, w0=-0.04013604953208452, w1=0.11302638095142079\n",
      "Gradient Descent(318/499): loss=0.3978590053984121, w0=-0.04019721173406399, w1=0.1131834036173767\n",
      "Gradient Descent(319/499): loss=0.3978424079273216, w0=-0.040258165847887516, w1=0.11334013507232908\n",
      "Gradient Descent(320/499): loss=0.3978259210218242, w0=-0.040318912288509104, w1=0.11349657665946912\n",
      "Gradient Descent(321/499): loss=0.39780954377659267, w0=-0.04037945147760986, w1=0.1136527297048442\n",
      "Gradient Descent(322/499): loss=0.39779327529460484, w0=-0.04043978384341089, w1=0.11380859551769018\n",
      "Gradient Descent(323/499): loss=0.39777711468705523, w0=-0.04049990982048966, w1=0.1139641753907569\n",
      "Gradient Descent(324/499): loss=0.3977610610732655, w0=-0.040559829849600035, w1=0.11411947060062726\n",
      "Gradient Descent(325/499): loss=0.3977451135805986, w0=-0.04061954437749563, w1=0.11427448240802986\n",
      "Gradient Descent(326/499): loss=0.397729271344373, w0=-0.04067905385675672, w1=0.11442921205814528\n",
      "Gradient Descent(327/499): loss=0.3977135335077779, w0=-0.04073835874562039, w1=0.11458366078090637\n",
      "Gradient Descent(328/499): loss=0.3976978992217902, w0=-0.04079745950781405, w1=0.11473782979129231\n",
      "Gradient Descent(329/499): loss=0.3976823676450922, w0=-0.040856356612392127, w1=0.1148917202896169\n",
      "Gradient Descent(330/499): loss=0.39766693794399116, w0=-0.04091505053357602, w1=0.11504533346181103\n",
      "Gradient Descent(331/499): loss=0.39765160929233956, w0=-0.040973541750597024, w1=0.11519867047969946\n",
      "Gradient Descent(332/499): loss=0.39763638087145575, w0=-0.04103183074754253, w1=0.11535173250127195\n",
      "Gradient Descent(333/499): loss=0.39762125187004665, w0=-0.04108991801320502, w1=0.11550452067094913\n",
      "Gradient Descent(334/499): loss=0.3976062214841319, w0=-0.04114780404093421, w1=0.11565703611984281\n",
      "Gradient Descent(335/499): loss=0.39759128891696727, w0=-0.041205489328491926, w1=0.11580927996601119\n",
      "Gradient Descent(336/499): loss=0.39757645337897085, w0=-0.04126297437791003, w1=0.11596125331470891\n",
      "Gradient Descent(337/499): loss=0.39756171408764956, w0=-0.04132025969535093, w1=0.11611295725863201\n",
      "Gradient Descent(338/499): loss=0.39754707026752617, w0=-0.04137734579097107, w1=0.116264392878158\n",
      "Gradient Descent(339/499): loss=0.3975325211500682, w0=-0.04143423317878692, w1=0.11641556124158113\n",
      "Gradient Descent(340/499): loss=0.3975180659736172, w0=-0.041490922376543816, w1=0.11656646340534282\n",
      "Gradient Descent(341/499): loss=0.3975037039833188, w0=-0.04154741390558722, w1=0.11671710041425758\n",
      "Gradient Descent(342/499): loss=0.39748943443105444, w0=-0.04160370829073675, w1=0.1168674733017342\n",
      "Gradient Descent(343/499): loss=0.39747525657537275, w0=-0.04165980606016251, w1=0.11701758308999272\n",
      "Gradient Descent(344/499): loss=0.39746116968142325, w0=-0.041715707745264105, w1=0.11716743079027675\n",
      "Gradient Descent(345/499): loss=0.39744717302088955, w0=-0.041771413880551916, w1=0.11731701740306176\n",
      "Gradient Descent(346/499): loss=0.39743326587192435, w0=-0.04182692500353091, w1=0.11746634391825898\n",
      "Gradient Descent(347/499): loss=0.39741944751908476, w0=-0.04188224165458663, w1=0.11761541131541531\n",
      "Gradient Descent(348/499): loss=0.3974057172532689, w0=-0.04193736437687367, w1=0.11776422056390908\n",
      "Gradient Descent(349/499): loss=0.39739207437165236, w0=-0.04199229371620624, w1=0.11791277262314198\n",
      "Gradient Descent(350/499): loss=0.3973785181776268, w0=-0.04204703022095111, w1=0.11806106844272704\n",
      "Gradient Descent(351/499): loss=0.39736504798073774, w0=-0.04210157444192259, w1=0.1182091089626728\n",
      "Gradient Descent(352/499): loss=0.39735166309662473, w0=-0.042155926932279784, w1=0.11835689511356376\n",
      "Gradient Descent(353/499): loss=0.397338362846961, w0=-0.042210088247425816, w1=0.11850442781673731\n",
      "Gradient Descent(354/499): loss=0.3973251465593944, w0=-0.042264058944909276, w1=0.11865170798445684\n",
      "Gradient Descent(355/499): loss=0.3973120135674887, w0=-0.042317839584327534, w1=0.1187987365200816\n",
      "Gradient Descent(356/499): loss=0.3972989632106662, w0=-0.04237143072723219, w1=0.11894551431823291\n",
      "Gradient Descent(357/499): loss=0.3972859948341508, w0=-0.04242483293703636, w1=0.11909204226495716\n",
      "Gradient Descent(358/499): loss=0.397273107788911, w0=-0.04247804677892401, w1=0.11923832123788543\n",
      "Gradient Descent(359/499): loss=0.39726030143160457, w0=-0.042531072819761005, w1=0.11938435210638991\n",
      "Gradient Descent(360/499): loss=0.39724757512452386, w0=-0.04258391162800825, w1=0.1195301357317371\n",
      "Gradient Descent(361/499): loss=0.39723492823553996, w0=-0.042636563773636386, w1=0.119675672967238\n",
      "Gradient Descent(362/499): loss=0.3972223601380509, w0=-0.04268902982804249, w1=0.11982096465839512\n",
      "Gradient Descent(363/499): loss=0.3972098702109268, w0=-0.042741310363968395, w1=0.11996601164304667\n",
      "Gradient Descent(364/499): loss=0.3971974578384582, w0=-0.04279340595542082, w1=0.12011081475150766\n",
      "Gradient Descent(365/499): loss=0.3971851224103041, w0=-0.042845317177593094, w1=0.12025537480670827\n",
      "Gradient Descent(366/499): loss=0.3971728633214398, w0=-0.042897044606788656, w1=0.1203996926243293\n",
      "Gradient Descent(367/499): loss=0.39716067997210747, w0=-0.042948588820346106, w1=0.12054376901293495\n",
      "Gradient Descent(368/499): loss=0.39714857176776425, w0=-0.04299995039656588, w1=0.12068760477410288\n",
      "Gradient Descent(369/499): loss=0.39713653811903427, w0=-0.04305112991463849, w1=0.12083120070255161\n",
      "Gradient Descent(370/499): loss=0.3971245784416586, w0=-0.04310212795457432, w1=0.12097455758626532\n",
      "Gradient Descent(371/499): loss=0.39711269215644684, w0=-0.043152945097134944, w1=0.12111767620661615\n",
      "Gradient Descent(372/499): loss=0.39710087868922944, w0=-0.04320358192376591, w1=0.12126055733848395\n",
      "Gradient Descent(373/499): loss=0.3970891374708098, w0=-0.04325403901653098, w1=0.12140320175037368\n",
      "Gradient Descent(374/499): loss=0.3970774679369177, w0=-0.043304316958047845, w1=0.12154561020453035\n",
      "Gradient Descent(375/499): loss=0.39706586952816286, w0=-0.0433544163314252, w1=0.1216877834570516\n",
      "Gradient Descent(376/499): loss=0.3970543416899886, w0=-0.043404337720201226, w1=0.12182972225799807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(377/499): loss=0.39704288387262715, w0=-0.043454081708283444, w1=0.1219714273515014\n",
      "Gradient Descent(378/499): loss=0.39703149553105405, w0=-0.04350364887988983, w1=0.1221128994758702\n",
      "Gradient Descent(379/499): loss=0.39702017612494406, w0=-0.04355303981949134, w1=0.12225413936369363\n",
      "Gradient Descent(380/499): loss=0.39700892511862707, w0=-0.04360225511175564, w1=0.12239514774194306\n",
      "Gradient Descent(381/499): loss=0.3969977419810451, w0=-0.04365129534149212, w1=0.12253592533207155\n",
      "Gradient Descent(382/499): loss=0.3969866261857083, w0=-0.04370016109359816, w1=0.12267647285011135\n",
      "Gradient Descent(383/499): loss=0.39697557721065346, w0=-0.04374885295300657, w1=0.12281679100676932\n",
      "Gradient Descent(384/499): loss=0.3969645945384012, w0=-0.04379737150463426, w1=0.12295688050752052\n",
      "Gradient Descent(385/499): loss=0.3969536776559147, w0=-0.043845717333332056, w1=0.1230967420526998\n",
      "Gradient Descent(386/499): loss=0.3969428260545582, w0=-0.04389389102383569, w1=0.12323637633759152\n",
      "Gradient Descent(387/499): loss=0.3969320392300565, w0=-0.04394189316071786, w1=0.12337578405251744\n",
      "Gradient Descent(388/499): loss=0.39692131668245434, w0=-0.043989724328341466, w1=0.12351496588292285\n",
      "Gradient Descent(389/499): loss=0.3969106579160767, w0=-0.04403738511081388, w1=0.12365392250946089\n",
      "Gradient Descent(390/499): loss=0.39690006243948917, w0=-0.04408487609194229, w1=0.12379265460807509\n",
      "Gradient Descent(391/499): loss=0.3968895297654589, w0=-0.04413219785519009, w1=0.12393116285008038\n",
      "Gradient Descent(392/499): loss=0.396879059410916, w0=-0.04417935098363434, w1=0.12406944790224221\n",
      "Gradient Descent(393/499): loss=0.39686865089691475, w0=-0.044226336059924126, w1=0.12420751042685425\n",
      "Gradient Descent(394/499): loss=0.39685830374859665, w0=-0.04427315366624003, w1=0.12434535108181438\n",
      "Gradient Descent(395/499): loss=0.3968480174951518, w0=-0.04431980438425447, w1=0.12448297052069912\n",
      "Gradient Descent(396/499): loss=0.39683779166978267, w0=-0.044366288795093044, w1=0.12462036939283658\n",
      "Gradient Descent(397/499): loss=0.396827625809667, w0=-0.04441260747929679, w1=0.12475754834337786\n",
      "Gradient Descent(398/499): loss=0.39681751945592125, w0=-0.04445876101678537, w1=0.12489450801336703\n",
      "Gradient Descent(399/499): loss=0.3968074721535651, w0=-0.04450474998682113, w1=0.12503124903980956\n",
      "Gradient Descent(400/499): loss=0.39679748345148513, w0=-0.04455057496797406, w1=0.1251677720557395\n",
      "Gradient Descent(401/499): loss=0.3967875529024002, w0=-0.04459623653808762, w1=0.12530407769028518\n",
      "Gradient Descent(402/499): loss=0.3967776800628263, w0=-0.044641735274245406, w1=0.12544016656873344\n",
      "Gradient Descent(403/499): loss=0.3967678644930418, w0=-0.044687071752738616, w1=0.12557603931259284\n",
      "Gradient Descent(404/499): loss=0.39675810575705356, w0=-0.04473224654903443, w1=0.12571169653965522\n",
      "Gradient Descent(405/499): loss=0.3967484034225629, w0=-0.04477726023774504, w1=0.12584713886405624\n",
      "Gradient Descent(406/499): loss=0.3967387570609316, w0=-0.04482211339259762, w1=0.12598236689633452\n",
      "Gradient Descent(407/499): loss=0.3967291662471498, w0=-0.04486680658640493, w1=0.12611738124348965\n",
      "Gradient Descent(408/499): loss=0.3967196305598022, w0=-0.04491134039103681, w1=0.1262521825090389\n",
      "Gradient Descent(409/499): loss=0.3967101495810359, w0=-0.044955715377392284, w1=0.12638677129307294\n",
      "Gradient Descent(410/499): loss=0.39670072289652786, w0=-0.0449999321153725, w1=0.12652114819231003\n",
      "Gradient Descent(411/499): loss=0.39669135009545364, w0=-0.045043991173854284, w1=0.1266553138001496\n",
      "Gradient Descent(412/499): loss=0.39668203077045516, w0=-0.04508789312066449, w1=0.12678926870672433\n",
      "Gradient Descent(413/499): loss=0.3966727645176095, w0=-0.045131638522554925, w1=0.12692301349895124\n",
      "Gradient Descent(414/499): loss=0.39666355093639843, w0=-0.045175227945178115, w1=0.12705654876058178\n",
      "Gradient Descent(415/499): loss=0.39665438962967736, w0=-0.04521866195306349, w1=0.12718987507225088\n",
      "Gradient Descent(416/499): loss=0.3966452802036452, w0=-0.045261941109594486, w1=0.12732299301152486\n",
      "Gradient Descent(417/499): loss=0.3966362222678139, w0=-0.04530506597698603, w1=0.12745590315294852\n",
      "Gradient Descent(418/499): loss=0.396627215434979, w0=-0.045348037116262854, w1=0.1275886060680911\n",
      "Gradient Descent(419/499): loss=0.39661825932118977, w0=-0.04539085508723826, w1=0.1277211023255914\n",
      "Gradient Descent(420/499): loss=0.3966093535457214, w0=-0.045433520448493595, w1=0.12785339249120176\n",
      "Gradient Descent(421/499): loss=0.3966004977310439, w0=-0.04547603375735824, w1=0.12798547712783145\n",
      "Gradient Descent(422/499): loss=0.3965916915027949, w0=-0.04551839556989021, w1=0.12811735679558883\n",
      "Gradient Descent(423/499): loss=0.39658293448975124, w0=-0.04556060644085731, w1=0.12824903205182284\n",
      "Gradient Descent(424/499): loss=0.39657422632380057, w0=-0.04560266692371886, w1=0.12838050345116345\n",
      "Gradient Descent(425/499): loss=0.3965655666399133, w0=-0.045644577570607886, w1=0.1285117715455615\n",
      "Gradient Descent(426/499): loss=0.39655695507611594, w0=-0.045686338932313976, w1=0.1286428368843275\n",
      "Gradient Descent(427/499): loss=0.3965483912734628, w0=-0.04572795155826651, w1=0.12877370001416966\n",
      "Gradient Descent(428/499): loss=0.3965398748760096, w0=-0.045769415996518535, w1=0.12890436147923115\n",
      "Gradient Descent(429/499): loss=0.3965314055307867, w0=-0.045810732793731004, w1=0.1290348218211267\n",
      "Gradient Descent(430/499): loss=0.3965229828877721, w0=-0.045851902495157644, w1=0.12916508157897813\n",
      "Gradient Descent(431/499): loss=0.3965146065998659, w0=-0.04589292564463019, w1=0.1292951412894495\n",
      "Gradient Descent(432/499): loss=0.3965062763228639, w0=-0.045933802784544155, w1=0.1294250014867812\n",
      "Gradient Descent(433/499): loss=0.3964979917154319, w0=-0.04597453445584505, w1=0.12955466270282356\n",
      "Gradient Descent(434/499): loss=0.39648975243907997, w0=-0.046015121198015006, w1=0.1296841254670696\n",
      "Gradient Descent(435/499): loss=0.39648155815813796, w0=-0.04605556354905995, w1=0.12981339030668712\n",
      "Gradient Descent(436/499): loss=0.3964734085397295, w0=-0.046095862045497094, w1=0.12994245774655028\n",
      "Gradient Descent(437/499): loss=0.3964653032537479, w0=-0.046136017222342916, w1=0.13007132830927026\n",
      "Gradient Descent(438/499): loss=0.39645724197283144, w0=-0.046176029613101556, w1=0.13020000251522543\n",
      "Gradient Descent(439/499): loss=0.39644922437233854, w0=-0.04621589974975359, w1=0.13032848088259089\n",
      "Gradient Descent(440/499): loss=0.396441250130325, w0=-0.046255628162745255, w1=0.13045676392736727\n",
      "Gradient Descent(441/499): loss=0.39643331892751854, w0=-0.04629521538097797, w1=0.13058485216340907\n",
      "Gradient Descent(442/499): loss=0.39642543044729595, w0=-0.0463346619317984, w1=0.13071274610245232\n",
      "Gradient Descent(443/499): loss=0.39641758437566016, w0=-0.046373968340988704, w1=0.13084044625414165\n",
      "Gradient Descent(444/499): loss=0.39640978040121605, w0=-0.04641313513275731, w1=0.1309679531260568\n",
      "Gradient Descent(445/499): loss=0.39640201821514803, w0=-0.046452162829729984, w1=0.13109526722373863\n",
      "Gradient Descent(446/499): loss=0.3963942975111972, w0=-0.04649105195294122, w1=0.13122238905071448\n",
      "Gradient Descent(447/499): loss=0.39638661798563907, w0=-0.04652980302182605, w1=0.13134931910852307\n",
      "Gradient Descent(448/499): loss=0.3963789793372607, w0=-0.04656841655421217, w1=0.1314760578967388\n",
      "Gradient Descent(449/499): loss=0.39637138126733884, w0=-0.046606893066312334, w1=0.13160260591299566\n",
      "Gradient Descent(450/499): loss=0.3963638234796182, w0=-0.04664523307271717, w1=0.1317289636530105\n",
      "Gradient Descent(451/499): loss=0.3963563056802893, w0=-0.04668343708638824, w1=0.13185513161060589\n",
      "Gradient Descent(452/499): loss=0.3963488275779674, w0=-0.04672150561865147, w1=0.1319811102777324\n",
      "Gradient Descent(453/499): loss=0.39634138888367093, w0=-0.04675943917919082, w1=0.1321069001444906\n",
      "Gradient Descent(454/499): loss=0.3963339893108006, w0=-0.04679723827604233, w1=0.1322325016991524\n",
      "Gradient Descent(455/499): loss=0.39632662857511797, w0=-0.046834903415588394, w1=0.13235791542818198\n",
      "Gradient Descent(456/499): loss=0.3963193063947254, w0=-0.046872435102552325, w1=0.13248314181625634\n",
      "Gradient Descent(457/499): loss=0.39631202249004516, w0=-0.04690983383999325, w1=0.13260818134628538\n",
      "Gradient Descent(458/499): loss=0.396304776583799, w0=-0.046947100129301235, w1=0.13273303449943155\n",
      "Gradient Descent(459/499): loss=0.39629756840098823, w0=-0.0469842344701927, w1=0.13285770175512907\n",
      "Gradient Descent(460/499): loss=0.3962903976688737, w0=-0.04702123736070608, w1=0.13298218359110275\n",
      "Gradient Descent(461/499): loss=0.39628326411695597, w0=-0.04705810929719773, w1=0.1331064804833865\n",
      "Gradient Descent(462/499): loss=0.39627616747695554, w0=-0.04709485077433818, w1=0.13323059290634123\n",
      "Gradient Descent(463/499): loss=0.39626910748279387, w0=-0.047131462285108436, w1=0.1333545213326726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(464/499): loss=0.39626208387057366, w0=-0.04716794432079676, w1=0.13347826623344822\n",
      "Gradient Descent(465/499): loss=0.3962550963785602, w0=-0.04720429737099551, w1=0.13360182807811466\n",
      "Gradient Descent(466/499): loss=0.3962481447471623, w0=-0.04724052192359828, w1=0.1337252073345139\n",
      "Gradient Descent(467/499): loss=0.3962412287189133, w0=-0.047276618464797276, w1=0.1338484044688996\n",
      "Gradient Descent(468/499): loss=0.39623434803845325, w0=-0.04731258747908085, w1=0.13397141994595288\n",
      "Gradient Descent(469/499): loss=0.39622750245250965, w0=-0.047348429449231344, w1=0.1340942542287979\n",
      "Gradient Descent(470/499): loss=0.39622069170988006, w0=-0.04738414485632304, w1=0.134216907779017\n",
      "Gradient Descent(471/499): loss=0.39621391556141333, w0=-0.047419734179720445, w1=0.1343393810566656\n",
      "Gradient Descent(472/499): loss=0.3962071737599925, w0=-0.0474551978970766, w1=0.13446167452028673\n",
      "Gradient Descent(473/499): loss=0.3962004660605163, w0=-0.04749053648433177, w1=0.13458378862692524\n",
      "Gradient Descent(474/499): loss=0.3961937922198823, w0=-0.047525750415712206, w1=0.1347057238321418\n",
      "Gradient Descent(475/499): loss=0.39618715199696936, w0=-0.047560840163729155, w1=0.13482748059002644\n",
      "Gradient Descent(476/499): loss=0.3961805451526197, w0=-0.047595806199178005, w1=0.134949059353212\n",
      "Gradient Descent(477/499): loss=0.39617397144962296, w0=-0.04763064899113767, w1=0.1350704605728871\n",
      "Gradient Descent(478/499): loss=0.39616743065269855, w0=-0.04766536900697007, w1=0.13519168469880902\n",
      "Gradient Descent(479/499): loss=0.3961609225284788, w0=-0.047699966712319915, w1=0.1353127321793161\n",
      "Gradient Descent(480/499): loss=0.3961544468454931, w0=-0.047734442571114456, w1=0.13543360346134012\n",
      "Gradient Descent(481/499): loss=0.39614800337415085, w0=-0.04776879704556365, w1=0.13555429899041813\n",
      "Gradient Descent(482/499): loss=0.39614159188672493, w0=-0.04780303059616022, w1=0.13567481921070432\n",
      "Gradient Descent(483/499): loss=0.3961352121573362, w0=-0.04783714368168011, w1=0.13579516456498147\n",
      "Gradient Descent(484/499): loss=0.3961288639619375, w0=-0.047871136759182906, w1=0.13591533549467213\n",
      "Gradient Descent(485/499): loss=0.3961225470782969, w0=-0.047905010284012554, w1=0.1360353324398497\n",
      "Gradient Descent(486/499): loss=0.39611626128598276, w0=-0.0479387647097981, w1=0.13615515583924911\n",
      "Gradient Descent(487/499): loss=0.39611000636634824, w0=-0.047972400488454664, w1=0.13627480613027745\n",
      "Gradient Descent(488/499): loss=0.39610378210251485, w0=-0.04800591807018449, w1=0.13639428374902424\n",
      "Gradient Descent(489/499): loss=0.3960975882793589, w0=-0.04803931790347819, w1=0.1365135891302715\n",
      "Gradient Descent(490/499): loss=0.3960914246834944, w0=-0.048072600435116054, w1=0.13663272270750373\n",
      "Gradient Descent(491/499): loss=0.39608529110325963, w0=-0.04810576611016955, w1=0.13675168491291748\n",
      "Gradient Descent(492/499): loss=0.39607918732870095, w0=-0.04813881537200289, w1=0.13687047617743084\n",
      "Gradient Descent(493/499): loss=0.3960731131515594, w0=-0.0481717486622748, w1=0.13698909693069278\n",
      "Gradient Descent(494/499): loss=0.39606706836525485, w0=-0.04820456642094031, w1=0.13710754760109214\n",
      "Gradient Descent(495/499): loss=0.39606105276487225, w0=-0.04823726908625276, w1=0.13722582861576657\n",
      "Gradient Descent(496/499): loss=0.39605506614714686, w0=-0.048269857094765826, w1=0.13734394040061124\n",
      "Gradient Descent(497/499): loss=0.39604910831045065, w0=-0.048302330881335745, w1=0.13746188338028725\n",
      "Gradient Descent(498/499): loss=0.39604317905477754, w0=-0.048334690879123565, w1=0.13757965797823013\n",
      "Gradient Descent(499/499): loss=0.3960372781817293, w0=-0.04836693751959758, w1=0.13769726461665782\n"
     ]
    }
   ],
   "source": [
    "max_iters = 500\n",
    "gamma = 0.08\n",
    "w_initial = np.zeros(len(tX[0]))\n",
    "\n",
    "weights,loss=least_squares_GD(y,tX, w_initial,max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y,tX,weights):\n",
    "    pred=predict_labels(weights, tX)\n",
    "    return np.sum(pred==y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.7020289514637226\n"
     ]
    }
   ],
   "source": [
    "print('accuracy=',accuracy(y,tX,weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
