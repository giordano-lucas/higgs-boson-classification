{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "import datetime\n",
    "from proj1_helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missing_values import *\n",
    "interpolator = LinearInterpolator()\n",
    "X = interpolator.interpolate(X.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "ditribution of the labels among the outliers :\n",
      "737 -1s and 2556 1s ratio= 0.2883411580594679\n",
      "Number of outliers removed :  3293\n"
     ]
    }
   ],
   "source": [
    "outlier_mask=np.linalg.norm(X,axis=1)<1500\n",
    "plt.hist(np.linalg.norm(X,axis=1))\n",
    "plt.show()\n",
    "print(outlier_mask.shape)\n",
    "# outliers removal\n",
    "X = X[outlier_mask]\n",
    "print('ditribution of the labels among the outliers :')\n",
    "print(np.sum(y[~outlier_mask]==-1),'-1s and',np.sum(y[~outlier_mask]==1),'1s ratio=',np.sum(y[~outlier_mask]==-1)/np.sum(y[~outlier_mask]==1) )\n",
    "y = y[outlier_mask]\n",
    "print('Number of outliers removed : ', np.sum(~outlier_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_y(y):\n",
    "    y[y<0] = 0\n",
    "    return y\n",
    "def inv_transform_y(y):\n",
    "    y[y==0] = -1\n",
    "    return y\n",
    "def logistic_error(y,X,w):\n",
    "    sigm=sigmoid(X@w)\n",
    "    pred = sigm > 0.5\n",
    "    return np.mean(y!=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y=np.array(transform_y(y))"
   ]
  },
  {
   "source": [
    "### Simple hyper parameter optimisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current iteration = 0, loss=29137.28482239975\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bb2629a8b078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'degree'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lambda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#call to the grid search function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbest_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyper_parameter_optimisation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/cross_validation.py\u001b[0m in \u001b[0;36mhyper_parameter_optimisation\u001b[0;34m(params, X, y)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m#w,loss_tr = ridge_regression(y_tr,x_tr,p['lambda'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         w,loss_tr  = reg_logistic_regression(y_tr,x_tr,\n\u001b[0;32m--> 165\u001b[0;31m             lambda_=p['lambda'],initial_w=np.zeros((x_tr.shape[1])),max_iters=2000,gamma=p['gamma'])\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;31m#get loss for val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         lambda y,tx,w: gradient_reg_logistic_regression(y,tx,w,lambda_))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#===================== Hepler functions =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(y, tx, initial_w, max_iters, gamma, compute_loss, compute_gradient)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgrd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current iteration = {i}, loss={l}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     94\u001b[0m     return gradient_descent(\n\u001b[1;32m     95\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         lambda y,tx,w: gradient_reg_logistic_regression(y,tx,w,lambda_))\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mloss_reg_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;34m\"\"\"compute the loss: negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# norm of w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgradient_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mloss_logistic_regression\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;34m\"\"\"compute the loss: negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cross_validation import hyper_parameter_optimisation\n",
    "degrees = np.arange(3,10)\n",
    "lambdas = np.logspace(-1, 2,2)\n",
    "gammas = [1e-6] #np.logspace(-6, -5, 3)\n",
    "params={'degree':degrees,'gamma':gammas,'lambda':lambdas,}\n",
    "#call to the grid search function\n",
    "best_param = hyper_parameter_optimisation(params,X,y)"
   ]
  },
  {
   "source": [
    "### K-fold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoch_grad_logit(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute a stochastic gradient from just few examples \n",
    "    n and their corresponding y_n labels.\n",
    "    \"\"\"   \n",
    "    by,bx = [b for b in batch_iter(y, tx, batch_size=4)][0]\n",
    "    return gradient_logistic_regression(by, bx, w)\n",
    "def stoch_logit(y,tx,initial_w,max_iters,gamma):\n",
    "    \"\"\"Logistic regression with gradient descent\"\"\"\n",
    "    return gradient_descent(\n",
    "        y, tx, initial_w,max_iters, gamma,\n",
    "        loss_logistic_regression,\n",
    "        stoch_grad_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "Current iteration = 0, loss=173286.60430277284\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n",
      "----------------\n",
      "%%%%%%%%%%%%%%%%\n",
      "****************\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-aba0d2fd386f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         gamma=p['gamma']),\n\u001b[0;32m---> 15\u001b[0;31m     loss_ft=logistic_error)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/cross_validation.py\u001b[0m in \u001b[0;36mgrid_search_cv\u001b[0;34m(params, X, y, k_fold, model, loss_ft)\u001b[0m\n\u001b[1;32m    168\u001b[0m             loss.append(do_cross_validation(\n\u001b[1;32m    169\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_CACHE_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 model=model,loss_ft=loss_ft))\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mgrid_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute the mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluated for {0} : loss = {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrid_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/cross_validation.py\u001b[0m in \u001b[0;36mdo_cross_validation\u001b[0;34m(k, k_fold_ind, X, y, params, _CACHE_, model, loss_ft)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcv_X_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_X_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m#fit on train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_y_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_X_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;31m#get loss for val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_y_te\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_X_te\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-aba0d2fd386f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y_tr, x_tr, p)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minitial_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         gamma=p['gamma']),\n\u001b[0m\u001b[1;32m     15\u001b[0m     loss_ft=logistic_error)\n",
      "\u001b[0;32m<ipython-input-16-bfd427b047f7>\u001b[0m in \u001b[0;36mstoch_logit\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss_logistic_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         stoch_grad_logit)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(y, tx, initial_w, max_iters, gamma, compute_loss, compute_gradient)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"****************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mgrd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-bfd427b047f7>\u001b[0m in \u001b[0;36mstoch_grad_logit\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0my_n\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"   \n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradient_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstoch_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-bfd427b047f7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0my_n\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"   \n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradient_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstoch_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/proj1_helpers.py\u001b[0m in \u001b[0;36mbatch_iter\u001b[0;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mshuffled_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cross_validation import grid_search_cv\n",
    "degrees = np.arange(5,7)\n",
    "lambdas = np.logspace(-1, 2,2)\n",
    "gammas = [1e-6] #np.logspace(-6, -5, 3)\n",
    "params={'degree':degrees,'gamma':gammas,'lambda':lambdas,}\n",
    "#call to the grid search function\n",
    "best_param = grid_search_cv(\n",
    "    params,X,y,k_fold=20,\n",
    "    model=lambda y_tr,x_tr,p: reg_logistic_regression(\n",
    "        y_tr,x_tr, \n",
    "        lambda_=p['lambda'],\n",
    "        initial_w=np.zeros((x_tr.shape[1])),\n",
    "        max_iters=1000,\n",
    "        gamma=p['gamma']),\n",
    "    loss_ft=logistic_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluated for {'degree': 1, 'lambda': 0.0001} : loss = 0.7628283770688195\n",
      "Evaluated for {'degree': 1, 'lambda': 0.00046415888336127773} : loss = 0.7628286927732513\n",
      "Evaluated for {'degree': 1, 'lambda': 0.002154434690031882} : loss = 0.7628287631461997\n",
      "Evaluated for {'degree': 1, 'lambda': 0.01} : loss = 0.762828784582916\n",
      "Evaluated for {'degree': 1, 'lambda': 0.046415888336127774} : loss = 0.7628288310779736\n",
      "Evaluated for {'degree': 1, 'lambda': 0.21544346900318823} : loss = 0.7628289213731477\n",
      "Evaluated for {'degree': 1, 'lambda': 1.0} : loss = 0.7628292018689985\n",
      "Evaluated for {'degree': 1, 'lambda': 4.641588833612772} : loss = 0.7628332335134178\n",
      "Evaluated for {'degree': 1, 'lambda': 21.54434690031882} : loss = 0.7628778517004712\n",
      "Evaluated for {'degree': 1, 'lambda': 100.0} : loss = 0.7631817333213131\n",
      "Evaluated for {'degree': 2, 'lambda': 0.0001} : loss = 0.760449951269683\n",
      "Evaluated for {'degree': 2, 'lambda': 0.00046415888336127773} : loss = 0.7604504579779428\n",
      "Evaluated for {'degree': 2, 'lambda': 0.002154434690031882} : loss = 0.7604505778916159\n",
      "Evaluated for {'degree': 2, 'lambda': 0.01} : loss = 0.760450604626616\n",
      "Evaluated for {'degree': 2, 'lambda': 0.046415888336127774} : loss = 0.7604506129068485\n",
      "Evaluated for {'degree': 2, 'lambda': 0.21544346900318823} : loss = 0.7604506517178988\n",
      "Evaluated for {'degree': 2, 'lambda': 1.0} : loss = 0.7604513429176009\n",
      "Evaluated for {'degree': 2, 'lambda': 4.641588833612772} : loss = 0.7604627621594349\n",
      "Evaluated for {'degree': 2, 'lambda': 21.54434690031882} : loss = 0.76057251366853\n",
      "Evaluated for {'degree': 2, 'lambda': 100.0} : loss = 0.7610064721551988\n",
      "Evaluated for {'degree': 3, 'lambda': 0.0001} : loss = 0.7557134631824481\n",
      "Evaluated for {'degree': 3, 'lambda': 0.00046415888336127773} : loss = 0.7557145502970268\n",
      "Evaluated for {'degree': 3, 'lambda': 0.002154434690031882} : loss = 0.7557148326431595\n",
      "Evaluated for {'degree': 3, 'lambda': 0.01} : loss = 0.7557149007660997\n",
      "Evaluated for {'degree': 3, 'lambda': 0.046415888336127774} : loss = 0.7557149629034788\n",
      "Evaluated for {'degree': 3, 'lambda': 0.21544346900318823} : loss = 0.7557154866078342\n",
      "Evaluated for {'degree': 3, 'lambda': 1.0} : loss = 0.7557223243578821\n",
      "Evaluated for {'degree': 3, 'lambda': 4.641588833612772} : loss = 0.7557695378151552\n",
      "Evaluated for {'degree': 3, 'lambda': 21.54434690031882} : loss = 0.7559693804898948\n",
      "Evaluated for {'degree': 3, 'lambda': 100.0} : loss = 0.7568175616462192\n",
      "Evaluated for {'degree': 4, 'lambda': 0.0001} : loss = 0.7515620939711256\n",
      "Evaluated for {'degree': 4, 'lambda': 0.00046415888336127773} : loss = 0.7515630703919813\n",
      "Evaluated for {'degree': 4, 'lambda': 0.002154434690031882} : loss = 0.7515633380642106\n",
      "Evaluated for {'degree': 4, 'lambda': 0.01} : loss = 0.7515636985126827\n",
      "Evaluated for {'degree': 4, 'lambda': 0.046415888336127774} : loss = 0.7515674459930084\n",
      "Evaluated for {'degree': 4, 'lambda': 0.21544346900318823} : loss = 0.7515846676672114\n",
      "Evaluated for {'degree': 4, 'lambda': 1.0} : loss = 0.7516218524478687\n",
      "Evaluated for {'degree': 4, 'lambda': 4.641588833612772} : loss = 0.7517501551297501\n",
      "Evaluated for {'degree': 4, 'lambda': 21.54434690031882} : loss = 0.7521233024972519\n",
      "Evaluated for {'degree': 4, 'lambda': 100.0} : loss = 0.7529093581218353\n",
      "Evaluated for {'degree': 5, 'lambda': 0.0001} : loss = 0.7468839697996764\n",
      "Evaluated for {'degree': 5, 'lambda': 0.00046415888336127773} : loss = 0.7468860694206884\n",
      "Evaluated for {'degree': 5, 'lambda': 0.002154434690031882} : loss = 0.7468940691048792\n",
      "Evaluated for {'degree': 5, 'lambda': 0.01} : loss = 0.7469180675332219\n",
      "Evaluated for {'degree': 5, 'lambda': 0.046415888336127774} : loss = 0.7469558922257198\n",
      "Evaluated for {'degree': 5, 'lambda': 0.21544346900318823} : loss = 0.7470604970159458\n",
      "Evaluated for {'degree': 5, 'lambda': 1.0} : loss = 0.7473201649232848\n",
      "Evaluated for {'degree': 5, 'lambda': 4.641588833612772} : loss = 0.7478541015529391\n",
      "Evaluated for {'degree': 5, 'lambda': 21.54434690031882} : loss = 0.7495899320981874\n",
      "Evaluated for {'degree': 5, 'lambda': 100.0} : loss = 0.7523553468657233\n",
      "Evaluated for {'degree': 6, 'lambda': 0.0001} : loss = 0.7437106389417741\n",
      "Evaluated for {'degree': 6, 'lambda': 0.00046415888336127773} : loss = 0.743715170773326\n",
      "Evaluated for {'degree': 6, 'lambda': 0.002154434690031882} : loss = 0.7437289736202448\n",
      "Evaluated for {'degree': 6, 'lambda': 0.01} : loss = 0.7438171187151268\n",
      "Evaluated for {'degree': 6, 'lambda': 0.046415888336127774} : loss = 0.7442272637962419\n",
      "Evaluated for {'degree': 6, 'lambda': 0.21544346900318823} : loss = 0.7449798225503798\n",
      "Evaluated for {'degree': 6, 'lambda': 1.0} : loss = 0.7461955879751927\n",
      "Evaluated for {'degree': 6, 'lambda': 4.641588833612772} : loss = 0.7476815310705176\n",
      "Evaluated for {'degree': 6, 'lambda': 21.54434690031882} : loss = 0.7490362066734939\n",
      "Evaluated for {'degree': 6, 'lambda': 100.0} : loss = 0.7513645197622838\n",
      "Evaluated for {'degree': 7, 'lambda': 0.0001} : loss = 0.7423346577155834\n",
      "Evaluated for {'degree': 7, 'lambda': 0.00046415888336127773} : loss = 0.7424347407823219\n",
      "Evaluated for {'degree': 7, 'lambda': 0.002154434690031882} : loss = 0.74284158442562\n",
      "Evaluated for {'degree': 7, 'lambda': 0.01} : loss = 0.7433587054518949\n",
      "Evaluated for {'degree': 7, 'lambda': 0.046415888336127774} : loss = 0.7438666854387149\n",
      "Evaluated for {'degree': 7, 'lambda': 0.21544346900318823} : loss = 0.74460779359763\n",
      "Evaluated for {'degree': 7, 'lambda': 1.0} : loss = 0.7455345912186424\n",
      "Evaluated for {'degree': 7, 'lambda': 4.641588833612772} : loss = 0.7470552650664409\n",
      "Evaluated for {'degree': 7, 'lambda': 21.54434690031882} : loss = 0.7489361281135453\n",
      "Evaluated for {'degree': 7, 'lambda': 100.0} : loss = 0.7510728391368732\n",
      "Evaluated for {'degree': 8, 'lambda': 0.0001} : loss = 0.7397399568877866\n",
      "Evaluated for {'degree': 8, 'lambda': 0.00046415888336127773} : loss = 0.7407197081316476\n",
      "Evaluated for {'degree': 8, 'lambda': 0.002154434690031882} : loss = 0.7419332838742948\n",
      "Evaluated for {'degree': 8, 'lambda': 0.01} : loss = 0.7428144543700315\n",
      "Evaluated for {'degree': 8, 'lambda': 0.046415888336127774} : loss = 0.743503222154161\n",
      "Evaluated for {'degree': 8, 'lambda': 0.21544346900318823} : loss = 0.7442522168666531\n",
      "Evaluated for {'degree': 8, 'lambda': 1.0} : loss = 0.7452748571659151\n",
      "Evaluated for {'degree': 8, 'lambda': 4.641588833612772} : loss = 0.7465982413612355\n",
      "Evaluated for {'degree': 8, 'lambda': 21.54434690031882} : loss = 0.7486291612438913\n",
      "Evaluated for {'degree': 8, 'lambda': 100.0} : loss = 0.7509705043430944\n",
      "======== Best test loss for parameters {'degree': 8, 'lambda': 0.0001} ========\n",
      "======== Best test loss score 0.7397399568877866 ========\n"
     ]
    }
   ],
   "source": [
    "degrees = np.arange(1,9)\n",
    "lambdas = np.logspace(-4, 2, 10)\n",
    "params={'degree':degrees,'lambda':lambdas}\n",
    "#call to the grid search function\n",
    "best_param = grid_search_cv(params,X,y,k_fold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current iteration = 0, loss=16697.01930269706\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bdba651762d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minitial_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     gamma=gamma)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         lambda y,tx,w: gradient_reg_logistic_regression(y,tx,w,lambda_))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#===================== Hepler functions =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(y, tx, initial_w, max_iters, gamma, compute_loss, compute_gradient)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mgrd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         lambda y,tx,w: gradient_reg_logistic_regression(y,tx,w,lambda_))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#===================== Hepler functions =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mgradient_reg_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m \u001b[0;31m# gradient of the normal w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradient_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgrad_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EPFL/Master/ML/Project 1/implementations.py\u001b[0m in \u001b[0;36mgradient_logistic_regression\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgradient_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_reg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from build_polynomial import PolynomialExpansion\n",
    "best_param = {'degree':6,'lambda':0.001, 'gamma':1e-6}\n",
    "degree  = best_param['degree']\n",
    "lambda_ = best_param['lambda']\n",
    "gamma=best_param['gamma']\n",
    "\n",
    "expanser = PolynomialExpansion(degree,with_interractions=True)\n",
    "tX       = expanser.expand(X)\n",
    "weights,loss_tr  = reg_logistic_regression(\n",
    "    y,tX,\n",
    "    lambda_=lambda_,\n",
    "    initial_w=np.zeros((tX.shape[1])),\n",
    "    max_iters=2000,\n",
    "    gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### For ridge reg ###########\n",
    "degree  = best_param['degree']\n",
    "lambda_ = best_param['lambda']\n",
    "expanser = PolynomialExpansion(\n",
    "    degree,with_interractions=True,with_scaler=True)\n",
    "tX       = expanser.expand(X)\n",
    "weights,loss_tr = ridge_regression(y,tX,lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic acc: 0.2259157624226309\n"
     ]
    }
   ],
   "source": [
    "print('logistic acc:' ,logistic_accuracy(y,tX,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x,y,w, print_=False):\n",
    "    pred=predict_labels(w, x)\n",
    "    false_positive=np.sum(pred>y)\n",
    "    false_negative=np.sum(pred<y)\n",
    "    true_positive=np.sum((pred==y) * (y==np.ones(len(y))*1))\n",
    "    true_negative=np.sum((pred==y) * (y==np.ones(len(y))*-1))\n",
    "    confusion_matrix=[[true_positive,false_positive],[false_negative,true_negative]]\n",
    "    if print_:\n",
    "        print(\"==============================\")\n",
    "        print('precision=',true_positive/(true_positive+false_positive))\n",
    "        print('accuracy=',(true_positive+true_negative)/len(pred))\n",
    "        print('recall=',true_positive/(true_positive+false_negative))\n",
    "        print('f1=',true_positive/(true_positive+0.5*(false_negative+false_positive)))\n",
    "        print(\"confusion matrix:\")\n",
    "        print(confusion_matrix[0])\n",
    "        print(confusion_matrix[1])\n",
    "        print(\"==============================\")   \n",
    "    \n",
    "    return (true_positive+true_negative)/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==============================\nprecision= 0.7587570915525946\naccuracy= 0.81642\nrecall= 0.6806821763339442\nf1= 0.7176022495831257\nconfusion matrix:\n[58312, 18540]\n[27355, 145793]\n==============================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.81642"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "evaluate_model(tX,y,weights, print_=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' \n",
    "y_sub, X_sub, ids_sub = load_csv_data(DATA_TEST_PATH)\n",
    "X_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply same transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_sub = interpolator.interpolate(X_sub)\n",
    "Xt_sub = expanser.expand(Xt_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  },
  {
   "source": [
    "### Least squares"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.3875172022990367"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "y_pred = predict_labels(weights, Xt_sub)\n",
    "np.sum(y_pred)/len(y_pred)"
   ]
  },
  {
   "source": [
    "### Logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predictor(X,w):\n",
    "    sigm=sigmoid(X@w)\n",
    "    pred = [ -1 if x<0.5 else 1 for x in sigm]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[autoreload of missing_values failed: Traceback (most recent call last):\n  File \"/Users/lucas/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n    superreload(m, reload, self.old_objects)\n  File \"/Users/lucas/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n    module = reload(module)\n  File \"/Users/lucas/anaconda3/lib/python3.7/imp.py\", line 314, in reload\n    return importlib.reload(module)\n  File \"/Users/lucas/anaconda3/lib/python3.7/importlib/__init__.py\", line 169, in reload\n    _bootstrap._exec(spec, module)\n  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/Users/lucas/Desktop/EPFL/Master/ML/Project 1/missing_values.py\", line 64\n    if self.weights[i]\n                     ^\nSyntaxError: invalid syntax\n]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'logistic_predictor' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8fe8355563a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#y_pred = predict_labels(weights, Xt_sub)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogistic_predictor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mXt_sub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logistic_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "#y_pred = predict_labels(weights, Xt_sub)\n",
    "y_pred=logistic_predictor( Xt_sub,weights)\n",
    "np.sum(y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save output for submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "create_csv_submission(ids_sub, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}